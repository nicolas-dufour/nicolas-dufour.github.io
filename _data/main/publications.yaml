- title: SCAM! Transferring humans between images with Semantic Cross Attention Modulation
  authors: <u><a href="#">Nicolas Dufour</a></u>, <a href="http://vicky.kalogeiton.info/">Vicky Kalogeiton</a>, <a href="https://davidpicard.github.io/">David Picard</a>
  date: 2022-10-10
  year: 2022
  journal: ECCV 2022
  pdf: https://arxiv.org/abs/2210.04883
  github: https://github.com/nicolas-dufour/SCAM
  website: scam.html
  thumbnail: /assets/publications/scam/thumbnail.png
  abstract: A large body of recent work targets semantically conditioned image generation. Most such methods focus on the narrower task of pose transfer and ignore the more challenging task of subject transfer that consists in not only transferring the pose but also the appearance and background. In this work, we introduce SCAM (Semantic Cross Attention Modulation), a system that encodes rich and diverse information in each semantic region of the image (including foreground and background), thus achieving precise generation with emphasis on fine details. This is enabled by the Semantic Attention Transformer Encoder that extracts multiple latent vectors for each semantic region, and the corresponding generator that exploits these multiple latents by using semantic cross attention modulation. It is trained only using a reconstruction setup, while subject transfer is performed at test time. Our analysis shows that our proposed architecture is successful at encoding the diversity of appearance in each semantic region. Extensive experiments on the iDesigner and CelebAMask-HD datasets show that SCAM outperforms SEAN and SPADE; moreover, it sets the new state of the art on subject transfer.
  bibtex: "@article{dufour2022scam, \n
   &nbsp;&nbsp; title={Scam! transferring humans between images with semantic cross attention modulation}, \n
   &nbsp;&nbsp; author={Dufour, Nicolas and Picard, David and Kalogeiton, Vicky}, \n
   &nbsp;&nbsp; booktitle={European Conference on Computer Vision}, \n
   &nbsp;&nbsp; pages={713--729}, \n
   &nbsp;&nbsp; year={2022}, \n
   &nbsp;&nbsp; organization={Springer} \n
   }"
- title: "Machine Learning for Brain Disorders: Transformers and Visual Transformers"
  authors: <a href="https://robincourant.github.io/info">Robin Courant</a>, Maika Edberg, <u><a href="#">Nicolas Dufour</a></u>, <a href="http://vicky.kalogeiton.info/">Vicky Kalogeiton</a>
  date: 2023-03-21
  year: 2023
  journal: Springer, Machine Learning for Brain Disorders
  pdf: https://arxiv.org/abs/2303.12068
  thumbnail: /assets/publications/transformers_mlbd/thumbnail.png
  abstract: Transformers were initially introduced for natural language processing (NLP) tasks, but fast they were adopted by most deep learning fields, including computer vision. They measure the relationships between pairs of input tokens (words in the case of text strings, parts of images for visual Transformers), termed attention. The cost is exponential with the number of tokens. For image classification, the most common Transformer Architecture uses only the Transformer Encoder in order to transform the various input tokens. However, there are also numerous other applications in which the decoder part of the traditional Transformer Architecture is also used. Here, we first introduce the Attention mechanism (Section 1), and then the Basic Transformer Block including the Vision Transformer (Section 2). Next, we discuss some improvements of visual Transformers to account for small datasets or less computation(Section 3). Finally, we introduce Visual Transformers applied to tasks other than image classification, such as detection, segmentation, generation and training without labels (Section 4) and other domains, such as video or multimodality using text or audio data (Section 5).
  bibtex: "@incollection{courant2012transformers, \n
    &nbsp;&nbsp; title={Transformers and Visual Transformers}, \n
    &nbsp;&nbsp; author={Courant, Robin and Edberg, Maika and Dufour, Nicolas and Kalogeiton, Vicky}, \n
    &nbsp;&nbsp; booktitle={Machine Learning for Brain Disorders}, \n
    &nbsp;&nbsp; pages={193--229}, \n
    &nbsp;&nbsp; year={2012}, \n
    &nbsp;&nbsp; publisher={Springer} \n
  }"
- title: Analysis of Classifier-Free Guidance Weight Schedulers
  authors: <a href="https://triocrossing.github.io/">Xi Wang</a>, <u><a href="#">Nicolas Dufour</a></u>, <a href="https://nefeliandreou.github.io/">Nefeli Andreou</a>, <a href="https://vabrevaya.github.io/">Victoria Fernandez Abrevaya</a>, <a href="https://www.lix.polytechnique.fr/vista/vista-member/marie-paule_cani/">Marie-Paule Cani</a>, <a href="https://davidpicard.github.io/">David Picard*</a>, <a href="http://vicky.kalogeiton.info/">Vicky Kalogeiton*</a>
  date: 2024-11-24
  year: 2024
  journal: TMLR
  pdf: https://arxiv.org/abs/2404.13040
  thumbnail: /assets/publications/guidance_schedulers/thumbnail.png
  abstract: Classifier-Free Guidance (CFG) enhances the quality and condition adherence of text-to-image diffusion models. It operates by combining the conditional and unconditional predictions using a fixed weight. However, recent works vary the weights throughout the diffusion process, reporting superior results but without providing any rationale or analysis. By conducting comprehensive experiments, this paper provides insights into CFG weight schedulers. Our findings suggest that simple, monotonically increasing weight schedulers consistently lead to improved performances, requiring merely a single line of code. In addition, more complex parametrized schedulers can be optimized for further improvement, but do not generalize across different models and tasks.
  bibtex: "@article{wang2024analysis, \n
    &nbsp;&nbsp; title={Analysis of Classifier-Free Guidance Weight Schedulers}, \n
    &nbsp;&nbsp; author={Xi Wang and Nicolas Dufour and Nefeli Andreou and Marie-Paule Cani\n
    &nbsp;&nbsp; and Victoria Fernandez Abrevaya and David Picard and Vicky Kalogeiton}, \n
    &nbsp;&nbsp; journal={TMLR}, \n
    &nbsp;&nbsp; year={2024} \n
  }"
- title: "OpenStreetView-5M: The Many Roads to Global Visual Geolocation"
  authors: <a href="https://gastruc.github.io/">Guillaume Astruc</a>*, <u><a href="#">Nicolas Dufour</a></u>*, <a href="https://imagine.enpc.fr/~siglidii/">Ioannis Siglidis</a>*, Constantin Aronssohn, Nacim Bouia, <a href="https://stephanie-fu.github.io/">Stephanie Fu</a>, <a href="https://romainloiseau.fr/">Romain Loiseau</a>, <a href="https://nv-nguyen.github.io/">Van Nguyen Nguyen</a>, <a href="https://imagine.enpc.fr/~raudec/">Charles Raude</a>, <a href="https://imagine.enpc.fr/~vincente/">Elliot Vincent</a>, Lintao XU, Hongyu Zhou, <a href="https://loiclandrieu.com/">Loic Landrieu</a>
  date: 2024-06-17
  year: 2024
  journal: CVPR 2024
  pdf: https://arxiv.org/abs/2404.18873
  github: https://github.com/gastruc/osv5m
  website: https://imagine.enpc.fr/~ioannis.siglidis/osv5m/
  thumbnail: /assets/publications/osv5m/thumbnail.png
  abstract: Determining the location of an image anywhere on Earth is a complex visual task, which makes it particularly relevant for evaluating computer vision algorithms. Yet, the absence of standard, large-scale, open-access datasets with reliably localizable images has limited its potential. To address this issue, we introduce OpenStreetView-5M, a large-scale, open-access dataset comprising over 5.1 million geo-referenced street view images, covering 225 countries and territories. In contrast to existing benchmarks, we enforce a strict train/test separation, allowing us to evaluate the relevance of learned geographical features beyond mere memorization. To demonstrate the utility of our dataset, we conduct an extensive benchmark of various state-of-the-art image encoders, spatial representations, and training strategies. 
  bibtex: "@article{astruc2024openstreetview5m, \n
    &nbsp;&nbsp; title={OpenStreetView-5M: The Many Roads to Global Visual Geolocation}, \n
    &nbsp;&nbsp; author={Guillaume Astruc and Nicolas Dufour and Ioannis Siglidis \n
    &nbsp;&nbsp; and Constantin Aronssohn and Nacim Bouia and Stephanie Fu and Romain Loiseau \n
    &nbsp;&nbsp; and Van Nguyen Nguyen and Charles Raude and Elliot Vincent and Lintao XU \n
    &nbsp;&nbsp; and Hongyu Zhou and Loic Landrieu}, \n
    &nbsp;&nbsp; journal={CVPR}, \n
    &nbsp;&nbsp; year={2024} \n
  }"
- title: "Don't drop your samples! Coherence-aware training benefits Conditional diffusion"
  authors: <u><a href="#">Nicolas Dufour</a></u>, <a href="https://scholar.google.com/citations?user=n_C2h-QAAAAJ&hl=fr">Victor Besnier</a>, <a href="http://vicky.kalogeiton.info/">Vicky Kalogeiton</a>, <a href="https://davidpicard.github.io/">David Picard</a>
  date: 2024-06-17
  year: 2024
  journal: CVPR 2024
  award: (Highlight)
  thumbnail: /assets/publications/cad_cvpr/thumbnail.png
  pdf: https://arxiv.org/abs/2405.20324
  github: https://github.com/nicolas-dufour/CAD
  website: cad.html
  abstract: Conditional diffusion models are powerful generative models that can leverage various types of conditional information, such as class labels, segmentation masks, or text captions. However, in many real-world scenarios, conditional information may be noisy or unreliable due to human annotation errors or weak alignment. In this paper, we propose the Coherence-Aware Diffusion (CAD), a novel method that integrates coherence in conditional information into diffusion models, allowing them to learn from noisy annotations without discarding data. We assume that each data point has an associated coherence score that reflects the quality of the conditional information. We then condition the diffusion model on both the conditional information and the coherence score. In this way, the model learns to ignore or discount the conditioning when the coherence is low. We show that CAD is theoretically sound and empirically effective on various conditional generation tasks. Moreover, we show that leveraging coherence generates realistic and diverse samples that respect conditional information better than models trained on cleaned datasets where samples with low coherence have been discarded.
  bibtex: "@article{dufour2024dont, \n
   &nbsp;&nbsp; title={Don't drop your samples! Coherence-aware training benefits Conditional diffusion}, \n
   &nbsp;&nbsp; author={Dufour, Nicolas and Besnier, Victor and Kalogeiton, Vicky and Picard, David}, \n
   &nbsp;&nbsp; booktitle={CVPR}, \n
   &nbsp;&nbsp; year={2024}, \n
   }"
- title: "E.T. the Exceptional Trajectories: Text-to-Camera-Trajectory Generation with Character Awareness."
  authors: <a href="https://robincourant.github.io/info">Robin Courant</a>, <u><a href="#">Nicolas Dufour</a></u>, <a href="https://triocrossing.github.io/">Xi Wang</a>,  <a href="https://people.irisa.fr/Marc.Christie/">Marc Christie</a>, <a href="http://vicky.kalogeiton.info/">Vicky Kalogeiton</a>
  date: 2024-07-01
  year: 2024
  journal: ECCV 2024
  thumbnail: /assets/publications/et/thumbnail.png
  pdf:  https://arxiv.org/abs/2407.01516
  github: https://github.com/robincourant/DIRECTOR
  website: https://www.lix.polytechnique.fr/vista/projects/2024_et_courant/
  abstract: Stories and emotions in movies emerge through the effect of well-thought-out directing decisions, in particular camera placement and movement over time. Crafting compelling camera motion trajectories remains a complex iterative process, even for skilful artists. To tackle this, in this paper, we propose a dataset called the Exceptional Trajectories (E.T.) with camera trajectories along with character information and textual captions encompassing description of both camera and character. To our knowledge, this is the first dataset of its kind. To show the potentialities of the E.T. dataset, we propose a diffusion-based approach, named DIRECTOR, which generates complex camera trajectories from textual captions that describe the relation and synchronisation between the camera and characters. Finally, to ensure a robust and accurate evaluation, we train CLaTr on the E.T. dataset, a language-trajectory feature representation used for metric calculation. Our work represents a significant advancement in democratizing the art of cinematography for common users.
  bibtex: "@article{courant2024et,\n
      &nbsp;&nbsp; author    = {Robin Courant and Nicolas Dufour and Xi Wang and Marc Christie and Vicky Kalogeiton},\n
      &nbsp;&nbsp; title     = {E.T. the Exceptional Trajectories: Text-to-camera-trajectory generation with character awareness},\n
      &nbsp;&nbsp; journal   = {arXiv},\n
      &nbsp;&nbsp; year      = {2024},\n
    }"

- title: "Around the World in 80 Timesteps: A Generative Approach to Global Visual Geolocation"
  authors: <u><a href="#">Nicolas Dufour</a></u>, <a href="https://davidpicard.github.io/">David Picard</a>, <a href="http://vicky.kalogeiton.info/">Vicky Kalogeiton</a>, <a href="https://loiclandrieu.com/">Loic Landrieu</a>
  date: 2025-06-10
  year: 2025
  journal: CVPR 2025
  thumbnail: /assets/publications/plonk/thumbnail.png
  pdf:  https://arxiv.org/abs/2412.06781
  github: https://github.com/nicolas-dufour/plonk
  website: plonk.html
  abstract: "Global visual geolocation predicts where an image was captured on Earth. Since images vary in how precisely they can be localized, this task inherently involves a significant degree of ambiguity. However, existing approaches are deterministic and overlook this aspect. In this paper, we aim to close the gap between traditional geolocalization and modern generative methods. We propose the first generative geolocation approach based on diffusion and Riemannian flow matching, where the denoising process operates directly on the Earth's surface. Our model achieves state-of-the-art performance on three visual geolocation benchmarks: OpenStreetView-5M, YFCC-100M, and iNat21. In addition, we introduce the task of probabilistic visual geolocation, where the model predicts a probability distribution over all possible locations instead of a single point. We introduce new metrics and baselines for this task, demonstrating the advantages of our diffusion-based approach."
  bibtex: "@article{dufour2024world80timestepsgenerative, \n
      &nbsp;&nbsp;  title           ={Around the World in 80 Timesteps: A Generative Approach to Global Visual Geolocation}, \n
      &nbsp;&nbsp;  author          ={Nicolas Dufour and David Picard and Vicky Kalogeiton and Loic Landrieu}, \n
      &nbsp;&nbsp;  year            ={2025}, \n
      &nbsp;&nbsp;  journal         ={CVPR}, \n
      }"

- title: "How far can we go with ImageNet for Text-to-Image generation?"
  authors: <a href="https://lucasdegeorge.github.io/">Lucas Degeorge</a>*, <a href="">Arijit Ghosh </a>*, <u><a href="#">Nicolas Dufour</a></u>, <a href="https://davidpicard.github.io/">David Picard</a><sup>†</sup>, <a href="http://vicky.kalogeiton.info/">Vicky Kalogeiton</a><sup>†</sup>
  date: 2025-07-01
  year: 2025
  journal: Arxiv Preprint
  thumbnail: /assets/publications/t2i_imagenet/thumbnail.png
  pdf:  https://arxiv.org/abs/2502.21318
  github: https://github.com/lucasdegeorge/T2I-ImageNet
  website: https://lucasdegeorge.github.io/projects/t2i_imagenet/
  abstract: "Recent text-to-image (T2I) generation models have achieved remarkable results by training on billion-scale datasets, following a `bigger is better' paradigm that prioritizes data quantity over quality. We challenge this established paradigm by demonstrating that strategic data augmentation of small, well-curated datasets can match or outperform models trained on massive web-scraped collections. Using only ImageNet enhanced with well-designed text and image augmentations, we achieve a +2 overall score over SD-XL on GenEval and +5 on DPGBench while using just 1/10th the parameters and 1/1000th the training images. Our results suggest that strategic data augmentation, rather than massive datasets, could offer a more sustainable path forward for T2I generation."
  bibtex: "@article{dufour2024world80timestepsgenerative, \n
      &nbsp;&nbsp;  title           ={How far can we go with ImageNet for Text-to-Image generation?}, \n
      &nbsp;&nbsp;  author          ={Lucas Degeorge and Arijit Ghosh and Nicolas Dufour and David Picard and Vicky Kalogeiton}, \n
      &nbsp;&nbsp;  year            ={2025}, \n
      &nbsp;&nbsp;  journal         ={arXiv}, \n
      }"
- title: "PoM: Efficient Image and Video Generation with the Polynomial Mixer"
  authors: <a href="https://davidpicard.github.io/">David Picard</a>, <u><a href="#">Nicolas Dufour</a></u>
  date: 2025-09-09
  year: 2025
  journal: Arxiv Preprint
  thumbnail: /assets/publications/pom/thumbnail.png
  pdf:  https://arxiv.org/abs/2411.12663
  github: https://github.com/davidpicard/Homm
  abstract: Diffusion models based on Multi-Head Attention (MHA) have become ubiquitous to generate high quality images and videos. However, encoding an image or a video as a sequence of patches results in costly attention patterns, as the requirements both in terms of memory and compute grow quadratically. To alleviate this problem, we propose a drop-in replacement for MHA called the Polynomial Mixer (PoM) that has the benefit of encoding the entire sequence into an explicit state. PoM has a linear complexity with respect to the number of tokens. This explicit state also allows us to generate frames in a sequential fashion, minimizing memory and compute requirement, while still being able to train in parallel. We show the Polynomial Mixer is a universal sequence-to-sequence approximator, just like regular MHA. We adapt several Diffusion Transformers (DiT) for generating images and videos with PoM replacing MHA, and we obtain high quality samples while using less computational resources
  bibtex: "@article{picard2024pom,\n
      &nbsp;&nbsp; author    = {David Picard and Nicolas Dufour},\n
      &nbsp;&nbsp; title     = {PoM: Efficient Image and Video Generation with the Polynomial Mixer},\n
      &nbsp;&nbsp; journal   = {arXiv},\n
      &nbsp;&nbsp; year      = {2024},\n
    }"

- title: "DIPSY: Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance"
  authors: Luc Boudier, Loris Manganelli, <a href="https://eleftsonis.github.io/">Eleftherios Tsonis</a>, <u><a href="#">Nicolas Dufour</a></u>, <a href="http://vicky.kalogeiton.info/">Vicky Kalogeiton</a>
  date: 2025-11-28
  year: 2025
  journal: BMVC 2025
  thumbnail: /assets/publications/dipsy/thumbnail.png
  pdf:  https://arxiv.org/abs/2509.22635
  github: https://github.com/eleftsonis/DIPSY
  website: https://www.lix.polytechnique.fr/vista/projects/2025_bmvc_dipsy/ 
  abstract: "Few-shot image classification remains challenging due to the limited availability of labeled examples. Recent approaches have explored generating synthetic training data using text-to-image diffusion models, but often require extensive model fine-tuning or external information sources. We present a novel training-free approach, called DIPSY, that leverages IP-Adapter for image-to-image translation to generate highly discriminative synthetic images using only the available few-shot examples. \n DIPSY introduces three key innovations: (1) an extended classifier-free guidance scheme that enables independent control over positive and negative image conditioning; (2) a class similarity-based sampling strategy that identifies effective contrastive examples; and (3) a simple yet effective pipeline that requires no model fine-tuning or external captioning and filtering. Experiments across ten benchmark datasets demonstrate that our approach achieves state-of-the-art or comparable performance, while eliminating the need for generative model adaptation or reliance on external tools for caption generation and image filtering. \n Our results highlight the effectiveness of leveraging dual image prompting with positive-negative guidance for generating class-discriminative features, particularly for fine-grained classification tasks. "
  bibtex: "@inproceedings{boudier2025dipsy,\n
      &nbsp;&nbsp; author    = {Luc Boudier and Loris Manganelli and Eleftherios Tsonis and Nicolas Dufour and Vicky Kalogeiton},\n
      &nbsp;&nbsp; title     = {DIPSY: Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance},\n
      &nbsp;&nbsp; booktitle = {BMVC},\n
      &nbsp;&nbsp; year      = {2025},\n
    }"

